---
title: Untitled
author: 
  - name: Bodo Burger
    #affiliation: LMU Munich
    #address: >
    #  First line
    #  Second line
    #email: \email{bb@example.com}
    #url: http://example.com
  #- name: Second Author
    #affiliation: Affiliation
date: 2018-00-00
output:
  html_document: 
    toc: yes
  github_document:
    toc: yes
  pdf_document:
    toc: yes
urlcolor: blue
#bibliography: "literature.bib"
#link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE,
                      cache = TRUE, cache.path = "cache/",
                      fig.path = "figures/file-name-")
library("intame")
library("ggplot2")
library("mlr")
library("ame")
library("patchwork")
theme_set(theme_light())
set.seed(4218)
```


```{r}
set.seed(4219)
n = 500
x = runif(n, min = 0, max = 1)
x1 = runif(n, min=0, max=1) + .5*x
x2 = runif(n, min=0, max=1) + .5*x
x_o_cor = runif(n, min=0, max=1) + .5*x
x_o_indep = runif(n, min=0, max=1) + .5*runif(n, 0, 1)
y1 = function(x) 4*x
y2 = function(x) -4 * cos(4*pi*x) * x + 4
y = y1(x1) + y2(x2) + rnorm(n, 0, .5) + 20*x_o_cor^2 + 20*x_o_indep^2
df = data.frame(y, x1, x2)
df.full = data.frame(y, x1, x2, x_o_cor, x_o_indep)
knitr::kable(head(df))
knitr::kable(cor(df.full)[,-1])
qplot(x1, y1(x1), geom = "line") + ggtitle("\"True effect\" for x1") +
qplot(x2, y2(x2), geom = "line") + ggtitle("\"True effect\" for x2")
```

```{r fit-neural-network}
lm.mod = lm(y ~ ., df)
plotPrediction(lm.mod, df, "x2")
lm.mod.full = lm(y ~ x1 + x2 + I(x_o_cor^2) + I(x_o_indep^2), df.full)
lm.mod.full

tsk = makeRegrTask(data = df.full, target = "y")
nnet.lrn = makeLearner("regr.nnet", skip = FALSE, size = 20, decay = 0.0001,
                       maxit = 1000, trace = FALSE)
nnet.mod = train(nnet.lrn, tsk)
plotPrediction(nnet.mod, tsk, "x2")
```

## ALE predictions to fit

```{r ALE}
nnet.ale1 = computeALE(nnet.mod$learner.model, df.full, "x1", grid.size = 40)
plot(nnet.ale1)
px1 = nnet.ale1$fp_x
pf1 = nnet.ale1$fp_f

nnet.ale2 = computeALE(nnet.mod$learner.model, df.full, "x2", grid.size = 40)
plot(nnet.ale2)
px2 = nnet.ale2$fp_x
pf2 = nnet.ale2$fp_f
```

## Version 0.1

```{r s3-system, eval=FALSE}
new_metric = function(x) {
  stopifnot(is.double(x))
  structure(x, class = "Date")
}

extract_metric_part_from_lm = function(mod) {
}

aggregate_parts = function()
```



```{r algo-0-1}
extract_metric_part_from_lm = function(mod) {
  summary(mod)$r.squared
}

aggregate_metric_parts = function(models, weights) {
  weighted.mean(vapply(models, extract_metric_part_from_lm, numeric(1)),
        weights)
}

compare_metric_with_optimum = function(metric, opt_metric) {
  metric > opt_metric
}

split_and_fit = function(x, f, method = "R2", threshold = .95, max_splits = 10) {
  l = length(x)
  splits = integer(0)
  metrics = numeric(0)
  mod_0 = lm(f ~ x, x = TRUE)
  opt_metric = extract_metric_part_from_lm(mod_0)
  opt_models = mod_0
  if (opt_metric >= threshold) return(list(models = list(mod_0),
                                           splits = NA,
                                           metrics = opt_metric))
  else {
  while(TRUE) {
    n_segments = length(splits) + 2
    splits_remaining = (2:(l-1))[!(2:(l-1) %in% splits)] # all remaining split.points
    metrics_new_split = numeric(length(splits_remaining))
    names(metrics_new_split) = splits_remaining
    opt_split = splits_remaining[1]
    for (split in splits_remaining) {
      x_tmp = vector("list", n_segments)
      f_tmp = vector("list", n_segments)
      mod_tmp = vector("list", n_segments)
      weights = numeric(n_segments)
      bounds = sort(c(1, l, splits, split))
      for (i in seq_len(n_segments)) {
        x_tmp[[i]] = x[bounds[i]:bounds[i+1]]
        f_tmp[[i]] = f[bounds[i]:bounds[i+1]]
        mod_tmp[[i]] = lm(f_tmp[[i]] ~ x_tmp[[i]], x = TRUE)
        weights[i] = length(x_tmp[[i]])
      }
      metric = aggregate_metric_parts(mod_tmp, weights)
      metrics_new_split[as.character(split)] = metric
      if (compare_metric_with_optimum(metric, opt_metric)) {
        opt_metric = metric
        opt_split = split
        opt_models = mod_tmp
      }
    }
    splits = c(splits, opt_split)
    metrics = c(metrics, opt_metric)
    if (opt_metric > threshold || length(splits) == max_splits) {
      print("Enough splits.")
      return(list(models = opt_models,
                  splits = splits,
                  metrics = metrics,
                  threshold = threshold))
    }
  }}
}

splits_plot_points = function(saf) {
  x = numeric(0)
  y = numeric(0)
  id = numeric(0)
  models = saf$models
  if (length(models) == 1) {
    x = models[[1]]$x[,2]
    y = models[[1]]$fitted.values
    id = rep(1, length(x))
  } else {
    for (i in seq_along(models)) {
      x_i = models[[i]]$x[,2]
      x = c(x, x_i)
      y = c(y, models[[i]]$fitted.values)
      id = c(id, rep(i, length(x_i)))
    }
  }
  return(data.frame(x, y, id))
}
```

```{r}
saf_x1 = split_and_fit(px1, pf1, .95)
saf_x1$splits
saf_x1$metrics
gg.data = splits_plot_points(saf_x1)
plot(nnet.ale1) + geom_line(data = gg.data, aes(x, y, group = id), col = "blue")

saf_x2 = split_and_fit(px2, pf2, .8, max_splits = 10)
plot(nnet.ale2) +
  geom_line(data = splits_plot_points(saf_x2), aes(x, y, group = id), col = "blue") +
  geom_vline(xintercept = px2[saf_x2$splits], linetype = 3, size = .4)
px2[saf_x2$splits]
saf_x2$metrics
```

## Remarks

### Preference for adjacent points

Comparing weighted means of the segments' R2 is important
because without it there was a strong preference for adjacent points 
(as two adjacent points yield an R2 of 1).

Further, due to starting at the left side left split points are preferred.
Problem?

### Slope influences splits when using R2

The higher the slope in a segment, the more likely it is being split

```{r different-slopes}
nnet.ale1_flat = computeALE(nnet.mod$learner.model, df.full, "x1", grid.size = 40)
plot(nnet.ale1_flat)
px1 = nnet.ale1$fp_x
pf1 = nnet.ale1$fp_f
```

### Slope incluences stopping criterium when using R2
