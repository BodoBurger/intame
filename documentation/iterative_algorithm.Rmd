---
title: Iterative partitioning
author: 
  - name: Bodo Burger
    #affiliation: LMU Munich
    #address: >
    #  First line
    #  Second line
    #email: \email{bb@example.com}
    #url: http://example.com
  #- name: Second Author
    #affiliation: Affiliation
date: 2018-00-00
output:
  html_document: 
    toc: yes
  github_document:
    toc: yes
  pdf_document:
    toc: yes
urlcolor: blue
#bibliography: "literature.bib"
#link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE,
                      cache = FALSE, cache.path = "cache/iter-algo")
library("intame")
library("ggplot2")
library("mlr")
library("ame")
library("patchwork")
theme_set(theme_light())
```

```{r}
y1 = function(x) 4*x
y2 = function(x) -4 * cos(4*pi*x) * x + 4
dgp = function(n, y1, y2) {
  x = runif(n, min = 0, max = 1)
  x1 = runif(n, min=0, max=1) + .5*x
  x2 = runif(n, min=0, max=1) + .5*x
  x_o_cor = runif(n, min=0, max=1) + .5*x
  x_o_indep = runif(n, min=0, max=1) + .5*runif(n, 0, 1)
  y = y1(x1) + y2(x2) + rnorm(n, 0, .5) + 20*x_o_cor^2 + 20*x_o_indep^2
  data.frame(y, x1, x2, x_o_cor, x_o_indep)
}
set.seed(4219)
df = dgp(n = 500, y1, y2)
knitr::kable(head(df))
knitr::kable(cor(df)[,-1])
qplot(df$x1, y1(df$x1), geom = "line") + ggtitle("\"True effect\" for x1") +
qplot(df$x2, y2(df$x2), geom = "line") + ggtitle("\"True effect\" for x2")
```

```{r fit-neural-network}
# lm.mod.omitted = lm(y ~ x1 + x2, df)
# lm.mod.omitted
lm.mod = lm(y ~ x1 + x2 + I(x_o_cor^2) + I(x_o_indep^2), df)
# lm.mod
df_test = dgp(1000, y1, y2)
mean((df_test$y - predict(lm.mod, df_test))^2) # test set MSE linear model

tsk = makeRegrTask(data = df, target = "y")
nnet.lrn = makeLearner("regr.nnet", skip = FALSE, size = 20, decay = 0.0001,
                       maxit = 1000, trace = FALSE)
nnet.mod = train(nnet.lrn, tsk)
performance(predict(nnet.mod, newdata = df_test), list(mse)) # test set MSE
plotPrediction(nnet.mod, tsk, "x2")$plot
```

## ALE predictions to fit

```{r ALE}
grid.size = 30
nnet.ale1 = computeALE(nnet.mod$learner.model, df, "x1", grid.size)
plot(nnet.ale1)
px1 = nnet.ale1$fp_x
pf1 = nnet.ale1$fp_f
nnet.ale2 = computeALE(nnet.mod$learner.model, df, "x2", grid.size)
plot(nnet.ale2)
px2 = nnet.ale2$fp_x
pf2 = nnet.ale2$fp_f
```

## Partition AL predictions

```{r}
saf_x1 = iterative_partition(px1, pf1, threshold = .95)
plot(saf_x1)
saf_x1

saf_x2 = iterative_partition(px2, pf2, "WMSR2", threshold = .9, max_splits = 10)
plot(saf_x2)
saf_x2

intame_x2 = intame(nnet.mod$learner.model, df, "x2", fe_grid_size = grid.size)
plot(intame_x2)
intame_x2

saf_x2.RSS = iterative_partition(px2, pf2, "WMRSS", threshold = .3, max_splits = 10)
plot(saf_x2.RSS)
saf_x2.RSS

saf_x2_non_greedy = iterative_partition(px2, pf2, "WMSR2", threshold = .95, max_splits = 10,
  greedy = FALSE)
plot(saf_x2_non_greedy)
saf_x2_non_greedy

plot(saf_x2_non_greedy) + plot(saf_x2)
```

## Ideas

- after splitting --> try all combinations of splits, reward split reduction

## Remarks

### Preference for adjacent, left-side points

Comparing weighted means of the segments' R2 is important
because without it there was a strong preference for adjacent points 
(as two adjacent points yield an R2 of 1).

Further, due to starting at the left side left split points are preferred.
Problem?

### A trend influences splits when using R2

A trend might incluence when the algorithm stops when using R2.
The higher the slope in a segment, the more likely it is being split. True?

```{r slope-data}
set.seed(4219)
x_min = 0; x_max = 10
n = 100
x1 = c(x_min, runif(n-2, x_min, x_max), x_max)
```

First example has a trend that dominates the cyclical component:

```{r slope-small-cycles}
y_flat = function(x) sin(.5*pi*x)
y_steep = function(x) sin(.5*pi*x) + x
ggplot() +
  geom_line(aes(x = x1, y = y_flat(x1), col = "y_flat")) +
  geom_line(aes(x = x1, y = y_steep(x1), col = "y_steep"))
saf_flat = iterative_partition(x1, y_flat(x1), "WMR2")
saf_steep = iterative_partition(x1, y_steep(x1), "WMR2")
plot(saf_flat) + ylim(-2, 11) +
plot(saf_steep) + ylim(-2, 11)
saf_flat.RSS = iterative_partition(x1, y_flat(x1), "WMRSS", .1)
saf_steep.RSS = iterative_partition(x1, y_steep(x1), "WMRSS", .1)
plot(saf_flat.RSS) + ylim(-1.5, 11) +
plot(saf_steep.RSS) + ylim(-1.5, 11)
```

"RSS" metric does not depend on the size of the trend.
"R2" metric depends on the size of the trend.

But what is the meaning of the threshold for "RSS"?
Stopping criterium unclear.

Second example has a bigger cyclical component:

```{r slope-big-cycles}
y_flat_bc = function(x) 4 * sin(.5*pi*x)
y_steep_bc = function(x) 4 * sin(.5*pi*x) + x
saf_flat_bc = iterative_partition(x1, y_flat_bc(x1), "WMR2")
saf_steep_bc = iterative_partition(x1, y_steep_bc(x1), "WMR2")
plot(saf_flat_bc) + ylim(-6, 15) +
plot(saf_steep_bc) + ylim(-6, 15)
saf_flat_bc_RSS = iterative_partition(x1, y_flat_bc(x1), "WMRSS")
saf_steep_bc_RSS = iterative_partition(x1, y_steep_bc(x1), "WMRSS")
plot(saf_flat_bc_RSS) + ylim(-6, 15) +
plot(saf_steep_bc_RSS) + ylim(-6, 15)
```

Same number of splits for both methods.

### Examples for saturated effect

### Other models

Here we try a tree-based method, but as long as the ALE plot is similar to
the one above the underlying method does not play a role in the results of
this algorithm.

```{r}
gbm.predict.fun = function(object, newdata) predict(object, newdata, n.trees = 1000)
gbm.lrn = makeLearner("regr.gbm", n.trees = 1000)
gbm.mod = train(gbm.lrn, tsk)
performance(predict(gbm.mod, newdata = df_test), list(mse)) # test set MSE

gbm.ale1 = computeALE(gbm.mod$learner.model, df, "x1", grid.size = 30,
  predict.fun = gbm.predict.fun)
plot(gbm.ale1)
gbm.ale2 = computeALE(gbm.mod$learner.model, df, "x2", grid.size = 30,
  predict.fun = gbm.predict.fun)
plot(gbm.ale2)

saf_x1 = iterative_partition(gbm.ale1$fp_x, gbm.ale1$fp_f, threshold = .9)
plot(saf_x1)
saf_x1

saf_x2 = iterative_partition(px2, pf2, method = "WMSR2", threshold = .9, max_splits = 20)
plot(saf_x2)
saf_x2
```

## Profiling / Benchmarks

```{r profiling, benchmarks}
library(profvis)
profvis(iterative_partition(x1, y_flat(x1), "WMR2"))
profvis(iterative_partition(x1, y_flat(x1), "WMRSS",threshold = .1, max_splits = 3, greedy = FALSE))

profvis(iterative_partition(px2, pf2, "WMSR2", threshold = .95, max_splits = 20,
  greedy = FALSE))

library(microbenchmark)
microbenchmark(
  iterative_partition(x1, y_flat(x1), "WMR2", max_splits = 2),
  iterative_partition(x1, y_flat(x1), "WMR2", max_splits = 2, greedy = FALSE),
  times = 5)

saf_steep = iterative_partition(x1, y_steep(x1), "WMR2")
saf_flat.RSS = iterative_partition(x1, y_flat(x1), "WMRSS", .1)
saf_steep.RSS = iterative_partition(x1, y_steep(x1), "WMRSS", .1)
```
