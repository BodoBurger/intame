---
title: Untitled
author: 
  - name: Bodo Burger
    #affiliation: LMU Munich
    #address: >
    #  First line
    #  Second line
    #email: \email{bb@example.com}
    #url: http://example.com
  #- name: Second Author
    #affiliation: Affiliation
date: 2018-00-00
output:
  html_document: 
    toc: yes
  github_document:
    toc: yes
  pdf_document:
    toc: yes
urlcolor: blue
#bibliography: "literature.bib"
#link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE,
                      cache = TRUE, cache.path = "cache/",
                      fig.path = "figures/file-name-")
library("intame")
library("ggplot2")
library("mlr")
library("ame")
library("patchwork")
theme_set(theme_light())
```

```{r}
y1 = function(x) 4*x
y2 = function(x) -4 * cos(4*pi*x) * x + 4
dgp = function(n, y1, y2) {
  x = runif(n, min = 0, max = 1)
  x1 = runif(n, min=0, max=1) + .5*x
  x2 = runif(n, min=0, max=1) + .5*x
  x_o_cor = runif(n, min=0, max=1) + .5*x
  x_o_indep = runif(n, min=0, max=1) + .5*runif(n, 0, 1)
  y = y1(x1) + y2(x2) + rnorm(n, 0, .5) + 20*x_o_cor^2 + 20*x_o_indep^2
  data.frame(y, x1, x2, x_o_cor, x_o_indep)
}
set.seed(4219)
df = dgp(n = 500, y1, y2)
knitr::kable(head(df))
knitr::kable(cor(df)[,-1])
qplot(df$x1, y1(df$x1), geom = "line") + ggtitle("\"True effect\" for x1") +
qplot(df$x2, y2(df$x2), geom = "line") + ggtitle("\"True effect\" for x2")
```

```{r fit-neural-network}
lm.mod.omitted = lm(y ~ x1 + x2, df)
lm.mod.omitted
lm.mod = lm(y ~ x1 + x2 + I(x_o_cor^2) + I(x_o_indep^2), df)
lm.mod

df_test = dgp(1000, y1, y2)
mean((df_test$y - predict(lm.mod, df_test))^2) # test set MSE

tsk = makeRegrTask(data = df, target = "y")
nnet.lrn = makeLearner("regr.nnet", skip = FALSE, size = 20, decay = 0.0001,
                       maxit = 1000, trace = FALSE)
nnet.mod = train(nnet.lrn, tsk)
plotPrediction(nnet.mod, tsk, "x2")
performance(predict(nnet.mod, newdata = df_test), list(mse)) # test set MSE
```

## ALE predictions to fit

```{r ALE}
nnet.ale1 = computeALE(nnet.mod$learner.model, df, "x1", grid.size = 30)
plot(nnet.ale1)
px1 = nnet.ale1$fp_x
pf1 = nnet.ale1$fp_f
nnet.ale2 = computeALE(nnet.mod$learner.model, df, "x2", grid.size = 30)
plot(nnet.ale2)
px2 = nnet.ale2$fp_x
pf2 = nnet.ale2$fp_f
```

## Partition AL predictions

```{r}
saf_x1 = split_and_fit(px1, pf1, threshold = .95)
plot(saf_x1)
saf_x1

saf_x2 = split_and_fit(px2, pf2, method = "WMSR2", threshold = .8, max_splits = 20)
plot(saf_x2)
saf_x2

saf_x2.RSS = split_and_fit(px2, pf2, method = "WMRSS", threshold = .95, max_splits = 20)
plot(saf_x2.RSS)
saf_x2.RSS
```

## Ideas

- after splitting --> try all combinations of splits, reward split reduction

## Remarks

### Preference for adjacent, left-side points

Comparing weighted means of the segments' R2 is important
because without it there was a strong preference for adjacent points 
(as two adjacent points yield an R2 of 1).

Further, due to starting at the left side left split points are preferred.
Problem?

### A trend influences splits when using R2

A trend might incluence when the algorithm stops when using R2.
The higher the slope in a segment, the more likely it is being split. True?

```{r slope-data}
set.seed(4219)
n = 100
x_min = 0; x_max = 10
x1 = c(x_min, runif(n-2, x_min, x_max), x_max)
```

First example has a trend that dominates the cyclical component:

```{r slope-small-cycles}
y_flat = function(x) sin(.5*pi*x)
y_steep = function(x) sin(.5*pi*x) + x
ggplot() +
  geom_line(aes(x = x1, y = y_flat(x1), col = "y_flat")) +
  geom_line(aes(x = x1, y = y_steep(x1), col = "y_steep"))
saf_flat = split_and_fit(x1, y_flat(x1), "WMR2")
saf_steep = split_and_fit(x1, y_steep(x1), "WMR2")
plot(saf_flat) + ylim(-1.5, 11) +
plot(saf_steep) + ylim(-1.5, 11)
saf_flat.RSS = split_and_fit(x1, y_flat(x1), "WMRSS")
saf_steep.RSS = split_and_fit(x1, y_steep(x1), "WMRSS")
plot(saf_flat.RSS) + ylim(-1.5, 11) +
plot(saf_steep.RSS) + ylim(-1.5, 11)
```

"RSS" uses that same amount of splits (none) regardless of the trend.
"R2" probably worse here as its metric depends on the size of the trend.

Second example has a bigger cyclical component:

```{r slope-big-cycles}
y_flat_bc = function(x) 4 * sin(.5*pi*x)
y_steep_bc = function(x) 4 * sin(.5*pi*x) + x
saf_flat_bc = split_and_fit(x1, y_flat_bc(x1), "WMR2")
saf_steep_bc = split_and_fit(x1, y_steep_bc(x1), "WMR2")
plot(saf_flat_bc) + ylim(-6, 15) +
plot(saf_steep_bc) + ylim(-6, 15)
saf_flat_bc_RSS = split_and_fit(x1, y_flat_bc(x1), "WMRSS")
saf_steep_bc_RSS = split_and_fit(x1, y_steep_bc(x1), "WMRSS")
plot(saf_flat_bc_RSS) + ylim(-6, 15) +
plot(saf_steep_bc_RSS) + ylim(-6, 15)
```

Same number of splits for both methods.

### Examples for saturated effect
