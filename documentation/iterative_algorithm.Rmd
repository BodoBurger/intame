---
title: Untitled
author: 
  - name: Bodo Burger
    #affiliation: LMU Munich
    #address: >
    #  First line
    #  Second line
    #email: \email{bb@example.com}
    #url: http://example.com
  #- name: Second Author
    #affiliation: Affiliation
date: 2018-00-00
output:
  html_document: 
    toc: yes
  github_document:
    toc: yes
  pdf_document:
    toc: yes
urlcolor: blue
#bibliography: "literature.bib"
#link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE,
                      cache = TRUE, cache.path = "cache/",
                      fig.path = "figures/file-name-")
library("intame")
library("ggplot2")
library("mlr")
library("ame")
library("patchwork")
theme_set(theme_light())
set.seed(4218)
```


```{r}
set.seed(4219)
n = 500
x = runif(n, min = 0, max = 1)
x1 = runif(n, min=0, max=1) + .5*x
x2 = runif(n, min=0, max=1) + .5*x
x_o_cor = runif(n, min=0, max=1) + .5*x
x_o_indep = runif(n, min=0, max=1) + .5*runif(n, 0, 1)
y1 = function(x) 4*x
y2 = function(x) -4 * cos(4*pi*x) * x + 4
y = y1(x1) + y2(x2) + rnorm(n, 0, .5) + 20*x_o_cor^2 + 20*x_o_indep^2
df = data.frame(y, x1, x2)
df.full = data.frame(y, x1, x2, x_o_cor, x_o_indep)
knitr::kable(head(df))
knitr::kable(cor(df.full)[,-1])
qplot(x1, y1(x1), geom = "line") + ggtitle("\"True effect\" for x1") +
qplot(x2, y2(x2), geom = "line") + ggtitle("\"True effect\" for x2")
```

```{r fit-neural-network}
lm.mod = lm(y ~ ., df)
plotPrediction(lm.mod, df, "x2")
lm.mod.full = lm(y ~ x1 + x2 + I(x_o_cor^2) + I(x_o_indep^2), df.full)
lm.mod.full

tsk = makeRegrTask(data = df.full, target = "y")
nnet.lrn = makeLearner("regr.nnet", skip = FALSE, size = 20, decay = 0.0001,
                       maxit = 1000, trace = FALSE)
nnet.mod = train(nnet.lrn, tsk)
plotPrediction(nnet.mod, tsk, "x2")
```

## ALE predictions to fit

```{r ALE}
nnet.ale1 = computeALE(nnet.mod$learner.model, df.full, "x1", grid.size = 40)
plot(nnet.ale1)
px1 = nnet.ale1$fp_x
pf1 = nnet.ale1$fp_f

nnet.ale2 = computeALE(nnet.mod$learner.model, df.full, "x2", grid.size = 40)
plot(nnet.ale2)
px2 = nnet.ale2$fp_x
pf2 = nnet.ale2$fp_f
```

## Partition AL predictions

```{r}
saf_x1 = split_and_fit(px1, pf1, threshold = .95)
plot(saf_x1)
saf_x1$splits
saf_x1$metrics

saf_x2 = split_and_fit(px2, pf2, method = "WMSRS", threshold = .95, max_splits = 5)
plot(saf_x2)
px2[saf_x2$splits]
saf_x2$metrics
```

## Ideas

- after splitting --> try all combinations of splits, reward split reduction

## Remarks

### Preference for adjacent points

Comparing weighted means of the segments' R2 is important
because without it there was a strong preference for adjacent points 
(as two adjacent points yield an R2 of 1).

Further, due to starting at the left side left split points are preferred.
Problem?

### Slope influences splits when using R2

The higher the slope in a segment, the more likely it is being split

```{r different-slopes}
nnet.ale1_flat = computeALE(nnet.mod$learner.model, df.full, "x1", grid.size = 40)
plot(nnet.ale1_flat)
px1 = nnet.ale1$fp_x
pf1 = nnet.ale1$fp_f
```

### Slope incluences stopping criterium when using R2
