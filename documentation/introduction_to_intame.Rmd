---
title: "Introduction to intame"
author: "Bodo Burger"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: yes
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Introduction to intame}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>",
  echo = TRUE, message = FALSE, warning = TRUE,
  cache = FALSE, cache.path = "cache/introduction_to_intame/")
library("intame")
library("ggplot2")
library("mlr")
library("ame")
theme_set(theme_light())
```

## Data generating process

<!-- ### Perfectly Separable, High Correlation -->

<!-- Regression example with two highly correlated features. -->

<!-- ```{r dgp-hi-cor} -->
<!-- set.seed(4321) -->
<!-- n = 500 -->
<!-- x = runif(n, min = 0, max = 1) -->
<!-- x1 = x + rnorm(n, 0, 0.05) -->
<!-- x2 = x + rnorm(n, 0, 0.05) -->
<!-- y2 = function(x) -4 * cos(4*pi*x) * x + 4 -->
<!-- y = x1 + y2(x2) + rnorm(n, 0, .5) -->
<!-- df = data.frame(y, x1, x2) -->
<!-- knitr::kable(head(df)) -->
<!-- print(cor(x1, x2)) -->
<!-- ``` -->

### Perfectly Separable, Moderate Correlation

```{r dgp-low-cor}
set.seed(4219)
n = 500
x = runif(n, min = 0, max = 1)
x1 = runif(n, min=0, max=1) + .5*x
x2 = runif(n, min=0, max=1) + .5*x
y2 = function(x) -4 * cos(4*pi*x) * x + 4
y = 4*x1 + y2(x2) + rnorm(n, 0, .5)
df = data.frame(y, x1, x2)
knitr::kable(head(df))
print(cor(x1, x2))
```

TODO: Print true reponses.

## Model - neural network

```{r fit-neural-network}
tsk = makeRegrTask(data = df, target = "y")
nnet.lrn = makeLearner("regr.nnet", skip = FALSE, size = 20, decay = 0.0001,
                       maxit = 1000, trace = FALSE)
nnet.mod = train(nnet.lrn, tsk)
plotPrediction(nnet.mod, tsk, "x1")$plot
plotPrediction(nnet.mod, tsk, "x2")$plot
```

The plot show the univariate response of the prediction function.
The blue line is created using loess regression.

## AME

```{r}
computeAME(nnet.mod, df, c("x1", "x2"))
```

TODO: Formel AME

AME for feature 1 corresponds to the "true" effect.
For feature 2, AME is less meaningful because of the non-monotonic effect.

## Comparison of ALE and PDeriv

### Feature 1

```{r feature-1}
ALE.x1 = computeALE(nnet.mod$learner.model, df, "x1", K = 50)
plot(ALE.x1, derivative = FALSE)
plot(ALE.x1, derivative = TRUE)
mean(ALE.x1$ale)
plot(computePD(nnet.mod$learner.model, df, "x1", derivative = FALSE))
PDeriv.x1 = computePD(nnet.mod$learner.model, df, "x1", derivative = TRUE)
plot(PDeriv.x1)
mean(PDeriv.x1$y.hat)
```

TODO: PD uniform=FALSE

Average of ALE nearly 4, while average of PDeriv slighty more off.
Does this indicate ALE is better at representing the marginal effect?

### Feature 2

```{r feature-2}
ALE.x2 = computeALE(nnet.mod$learner.model, df, "x2", K = 100)
plot(ALE.x2, derivative = TRUE)
PDeriv.x2 = computePD(nnet.mod$learner.model, df, "x2", derivative = TRUE)
plot(PDeriv.x2)
# analytical derivation of the additive part of feature 2:
y2_partial = function(x) 4 * sin(4*pi*x) * 4*pi * x - 4 * cos(4*pi*x)
mean(y2_partial(x2))
mean(ALE.x2$ale)
mean(PDeriv.x2$y.hat)
```

AME and average ALE correspond to "true" effect (= average partial derivative).
Again, PDeriv is "worse". *Is this analysis correct?*

## Find intervals of similar MEs

### Feature 2

```{r}
breaks = partition(ALE.x2$ale.x, ALE.x2$ale, 5)
plot(ALE.x2, derivative = FALSE) + geom_vline(xintercept = breaks)
plot(ALE.x2) + geom_vline(xintercept = partition(ALE.x2$ale.x, ALE.x2$ale, 5,
                                                 part.method = "cluster"))
```

rpart returns reasonable intervals. Clustering does not work here.

#### Compute AME given the intervals

```{r}
AME.x2 = intame(nnet.mod$learner.model, df, "x2", breaks = breaks)
AME.x2
plot(AME.x2)
```

We check the result by calculating the "true" effect in each interval
using the analytically calculated partial derivative.

```{r}
bounds = unique(c(min(x2), sort(breaks), max(x2) + 0.00001))
l = length(bounds) - 1
true.effects = numeric(l)
for (i in 1:l) {
  selection = x2 >= bounds[i] & x2 < bounds[i+1]
  df.interval = df[selection,]
  true.effects[i] = mean(y2_partial(x2[selection]))
}
AME.x2
true.effects
```

Checks out.

#### Add break point "manually"

An additional break point at around .25 might be sensible. We add it and calculate
AMEs for each interval again.

```{r}
AME.x2.add.break.point = intame(nnet.mod$learner.model, df, "x2",
                                            breaks = c(0.25, breaks))
AME.x2.add.break.point$AME
plot(AME.x2.add.break.point)
```

#### Fully automatic

*intame* calculates breaks points by default:

```{r}
AME.x2.auto = intame(nnet.mod$learner.model, df, "x2")
AME.x2.auto
plot(AME.x2.auto)
AME.x2.auto$bounds
```

#### Use PDeriv

```{r}
AME.x2.PDeriv = intame(nnet.mod$learner.model, df, "x2",
                                method = "PDeriv")
AME.x2.PDeriv
plot(AME.x2.PDeriv) + ggtitle("Partial Derivative")
```

Less sensible break points? Check true effect for each interval:

```{r}
bounds = AME.x2.PDeriv$bounds
l = length(bounds) - 1
true.effects = numeric(l)
for (i in 1:l) {
  selection = x2 >= bounds[i] & x2 < bounds[i+1]
  df.interval = df[selection,]
  true.effects[i] = mean(y2_partial(x2[selection]))
}
true.effects
```

Matches.

#### Local PDeriv

```{r}
AME.x2.LDeriv = intame(nnet.mod$learner.model, df, "x2",
                                method = "PDeriv", l = 40)
plot(AME.x2.LDeriv) + ggtitle("Local Partial Derivative, l = 40")
```

No difference to PDeriv.

#### Weighted PDeriv

```{r}
AME.x2.WDeriv = intame(nnet.mod$learner.model, df, "x2",
                                method = "PDeriv", w = 4)
plot(AME.x2.WDeriv) + ggtitle("Weighted Partial Derivative, w = 4")
```

No difference to PDeriv.
